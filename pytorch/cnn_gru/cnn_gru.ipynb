{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DmjQOcbTZCot"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "wOuwWIdy_O6Q"
   },
   "outputs": [],
   "source": [
    "from numpy import bincount\n",
    "from torch.optim import AdamW\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm as progress\n",
    "from torch.nn.utils import clip_grad_norm_ \n",
    "from pandas import read_csv, DataFrame, concat\n",
    "from torch.nn.functional import relu, max_pool1d\n",
    "from torch import IntTensor, FloatTensor, no_grad\n",
    "from torch import device, cuda, save, load, sigmoid  \n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn import ModuleList, Embedding, Dropout, Conv1d, GRU, Linear, BCELoss\n",
    "from torch.utils.data import TensorDataset, DataLoader, WeightedRandomSampler, SequentialSampler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, precision_recall_curve, auc, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Constant Variables\n",
    "batch_size = 256\n",
    "device = device(\"cuda\" if cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WkGsTT_XVqcP"
   },
   "source": [
    "# Loading SMS Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "GLwOwHC5CwTh",
    "outputId": "83c505f2-75c1-4eec-a6de-8d5d1f6a992b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1618</th>\n",
       "      <td>[3, 2772, 893, 1549, 3347, 531, 123, 1315, 449...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>[57, 1038, 1039, 325, 3, 218, 651, 57, 16, 429...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4683</th>\n",
       "      <td>[622, 554, 624, 15, 825, 283, 622, 15, 825, 28...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3337</th>\n",
       "      <td>[156, 15, 57, 1652, 463, 148, 17, 17, 0, 0, 0,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2139</th>\n",
       "      <td>[660, 211, 539, 873, 736, 159, 17, 17, 0, 0, 0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  type\n",
       "1618  [3, 2772, 893, 1549, 3347, 531, 123, 1315, 449...     0\n",
       "878   [57, 1038, 1039, 325, 3, 218, 651, 57, 16, 429...     1\n",
       "4683  [622, 554, 624, 15, 825, 283, 622, 15, 825, 28...     0\n",
       "3337  [156, 15, 57, 1652, 463, 148, 17, 17, 0, 0, 0,...     0\n",
       "2139  [660, 211, 539, 873, 736, 159, 17, 17, 0, 0, 0...     0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = read_csv(\"../../data/sms.csv\")\n",
    "df = df.drop(columns=[\"Unnamed: 0\"])\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AKRFUAkLk7y_"
   },
   "source": [
    "# Preparing SMS Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "V-qDsVR5lBBK"
   },
   "outputs": [],
   "source": [
    "# Load & Prepare Data\n",
    "df[\"text\"] = df[\"text\"].apply(lambda sms: [int(word) for word in sms[1:-1].split(\", \")])\n",
    "df[\"type\"] = df[\"type\"].apply(lambda label: int(label))\n",
    "\n",
    "# # Train / Valid / Test Split\n",
    "x_train, x_test, y_train, y_test = train_test_split(df[\"text\"], df[\"type\"], test_size=0.30, stratify=df[\"type\"], random_state=2022)\n",
    "x_valid, x_test, y_valid, y_test = train_test_split(x_test, y_test, test_size=0.50, stratify=y_test, random_state=2022)\n",
    "\n",
    "# Training weight\n",
    "train_weight = (1./bincount(y_train))[y_train]\n",
    "\n",
    "# Train / Valid / Test TensorDataset\n",
    "train_dataset = TensorDataset(IntTensor(x_train.to_list()), FloatTensor(y_train.to_list()))\n",
    "valid_dataset =  TensorDataset(IntTensor(x_valid.to_list()), FloatTensor(y_valid.to_list()))\n",
    "test_dataset = TensorDataset(IntTensor(x_test.to_list()), FloatTensor(y_test.to_list()))\n",
    "\n",
    "# Train / Test / Valid Loaders\n",
    "train_loader = DataLoader(train_dataset, sampler=WeightedRandomSampler(train_weight, len(train_weight)-1), batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_dataset, sampler=SequentialSampler(valid_dataset), batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQ-taMS4Acwl"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "gzyU9G4PAd3U"
   },
   "outputs": [],
   "source": [
    "class SMSClassifier(ModuleList):\n",
    "    def __init__(self, path):\n",
    "        super(SMSClassifier, self).__init__()\n",
    "        self.path = path\n",
    "        self.embedding = Embedding(num_embeddings=178348, embedding_dim=8, padding_idx=0)\n",
    "        self.dropout1 = Dropout(0.5)\n",
    "        self.conv1 = Conv1d(300, 200, kernel_size=3, stride=2, padding=1)\n",
    "        self.dropout2 = Dropout(0.5)\n",
    "        self.conv2 = Conv1d(200, 100, kernel_size=3, stride=2, padding=1)\n",
    "        self.dropout3 = Dropout(0.5)\n",
    "        self.conv3 = Conv1d(100, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.dropout4 = Dropout(0.5)\n",
    "        self.gru1 = GRU(input_size=1, hidden_size=8, batch_first=True, bidirectional=True)\n",
    "        self.dropout5 = Dropout(0.5)\n",
    "        self.gru2 = GRU(input_size=8, hidden_size=16, batch_first=True, bidirectional=True)\n",
    "        self.dropout6 = Dropout(0.5)\n",
    "        self.gru3 = GRU(input_size=16, hidden_size=32, batch_first=True, bidirectional=True)\n",
    "        self.dropout7 = Dropout(0.5)\n",
    "        self.fc1 = Linear(64, 32)\n",
    "        self.fc2 = Linear(32, 16)\n",
    "        self.fc3 = Linear(16, 1)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        sentence = self.embedding(sentence)\n",
    "        sentence = self.dropout1(sentence)\n",
    "        sentence = self.conv1(sentence)\n",
    "        sentence = relu(sentence, inplace=False)\n",
    "        sentence = max_pool1d(sentence, kernel_size=3, stride=2, padding=1)\n",
    "        sentence = self.dropout2(sentence)\n",
    "        sentence = self.conv2(sentence)\n",
    "        sentence = relu(sentence, inplace=False)\n",
    "        sentence = max_pool1d(sentence, kernel_size=3, stride=2, padding=1)\n",
    "        sentence = self.dropout3(sentence)\n",
    "        sentence = self.conv3(sentence)\n",
    "        sentence = relu(sentence, inplace=False)\n",
    "        sentence = max_pool1d(sentence, kernel_size=3, stride=2, padding=1)\n",
    "        sentence = self.dropout4(sentence)\n",
    "        sentence, (hidden, cell) = self.gru1(sentence)\n",
    "        sentence = relu(sentence, inplace=False)\n",
    "        sentence = max_pool1d(sentence, kernel_size=3, stride=2, padding=1)\n",
    "        sentence = self.dropout5(sentence)\n",
    "        sentence, (hidden, cell) = self.gru2(sentence)\n",
    "        sentence = relu(sentence, inplace=False)\n",
    "        sentence = max_pool1d(sentence, kernel_size=3, stride=2, padding=1)\n",
    "        sentence = self.dropout6(sentence)\n",
    "        sentence, (hidden, cell) = self.gru3(sentence)\n",
    "        sentence = relu(sentence, inplace=False)\n",
    "        sentence = max_pool1d(sentence, kernel_size=32, stride=64, padding=1)\n",
    "        sentence = sentence.view(-1, 64)\n",
    "        sentence = self.dropout7(sentence)\n",
    "        sentence = self.fc1(sentence)\n",
    "        sentence = relu(sentence, inplace=True)\n",
    "        sentence = self.fc2(sentence)\n",
    "        sentence = relu(sentence, inplace=True)\n",
    "        sentence = self.fc3(sentence).squeeze()\n",
    "        return sigmoid(sentence)\n",
    "\n",
    "    def fit(self, loader, epochs, optim, loss_fn):\n",
    "        train_loss, valid_loss = [1], [1]\n",
    "        for epoch in (tracker := progress(range(epochs))):\n",
    "            tracker.set_description(f\"Epoch #{epoch+1}\")\n",
    "            # Train Step\n",
    "            train_loss.append(self.train_step(loader[0], optim, loss_fn))\n",
    "            # Eval Step\n",
    "            valid_loss.append(loss := self.eval_step(loader[1], loss_fn))\n",
    "            # Updating tracker\n",
    "            if epoch == epochs-1:\n",
    "                for step, loss in enumerate(valid_loss):\n",
    "                    if loss == min(valid_loss):\n",
    "                        tracker.set_description(f\"Best Epoch #{step}\")\n",
    "                        tracker.set_postfix_str(f\"Training Loss = {train_loss[step]}, Validiation Loss = {valid_loss[step]}\")\n",
    "            else:\n",
    "                tracker.set_postfix_str(f\"Training Loss = {train_loss[-1]}, Validiation Loss = {valid_loss[-1]}\")\n",
    "            # Saving Model If Performance Improves\n",
    "            if loss <= min(valid_loss):\n",
    "                self.save()\n",
    "            \n",
    "        # Plotting Results\n",
    "        fig, ax = plt.subplots(figsize=(15, 5), dpi=300)\n",
    "        ax.set_title(\"Training Loss (Blue) vs Validation Loss (Red)\")\n",
    "        ax.plot(train_loss, \"b\")\n",
    "        ax.plot(valid_loss, \"r\")\n",
    "        ax.set_xlim((0, len(train_loss)-1))\n",
    "        ax.set_ylim((0, 1))\n",
    "\n",
    "    def train_step(self, loader, optim, loss_fn):\n",
    "        losses = []\n",
    "        self.train()\n",
    "        for step, batch in enumerate(loader):\n",
    "            self.zero_grad()\n",
    "            # Training Step\n",
    "            prediction = self.forward(batch[0].to(device))\n",
    "            loss = loss_fn(prediction, batch[1].type(FloatTensor).to(device))\n",
    "            # Recording Training Result\n",
    "            losses.append(loss.item())\n",
    "            # Refresh Step\n",
    "            loss.backward()\n",
    "            clip_grad_norm_(self.parameters(), max_norm=1.0, norm_type=1)\n",
    "            optim.step()\n",
    "        return sum(losses)/len(losses)\n",
    "\n",
    "    def eval_step(self, loader, loss_fn):\n",
    "        losses = []\n",
    "        self.eval()\n",
    "        with no_grad():\n",
    "            for step, batch in enumerate(loader):\n",
    "                # Evaluation Step\n",
    "                prediction = self.forward(batch[0].to(device))\n",
    "                loss = loss_fn(prediction, batch[1].type(FloatTensor).to(device))\n",
    "                # Recording Evaluation Result\n",
    "                losses.append(loss.item())       \n",
    "        return sum(losses)/len(losses)\n",
    "\n",
    "    def evaluate(self, loader):\n",
    "        self.load()\n",
    "        y_true, y_pred = [], []\n",
    "        self.eval()\n",
    "        with no_grad():\n",
    "            for step, batch in enumerate(loader):\n",
    "                y_pred.extend([int(pred > 0.5) for pred in self.forward(batch[0].to(device)).cpu().numpy()])\n",
    "                y_true.extend(batch[1])\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "        rc1, rc2, rc3 = precision_recall_curve(y_true, y_pred)\n",
    "        pr_auc = auc(rc2,  rc1)\n",
    "        print(f\"Accuracy: {accuracy}\\nPrecision: {precision}\\nRecall: {recall}\\nPR_AUC: {pr_auc}\")\n",
    "        ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_true, y_pred), display_labels=[0, 1]).plot()\n",
    "        plt.show()\n",
    "        return accuracy, precision, recall, pr_auc\n",
    "    \n",
    "    def saveResults(self, lib, nam, typ, acc, pre, rec, pr):\n",
    "        df = read_csv(\"../../data/results.csv\")\n",
    "        df2 = DataFrame([[lib, nam, typ, acc, pre, rec, pr]], \n",
    "                        columns=[\"Library\", \"Model\", \"Type\", \"Accuracy\", \"Precision\", \"Recall\", \"PR_AUC\"])\n",
    "        df = concat([df2, df])\n",
    "        if \"Unnamed: 0\" in df:\n",
    "            df = df.drop(columns=[\"Unnamed: 0\"])\n",
    "        df.to_csv(\"../../data/results.csv\")\n",
    "\n",
    "    def save(self):\n",
    "        save(self.state_dict(), self.path)\n",
    "\n",
    "    def load(self):\n",
    "        self.load_state_dict(load(self.path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TVxlOXjb5aWF"
   },
   "source": [
    "# Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wC-Fj8bn5Lwz",
    "outputId": "01ebacc1-211a-4e7c-e327-ad4e5d53c1c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMSClassifier(\n",
      "  (embedding): Embedding(178348, 8, padding_idx=0)\n",
      "  (dropout1): Dropout(p=0.5, inplace=False)\n",
      "  (conv1): Conv1d(300, 200, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "  (dropout2): Dropout(p=0.5, inplace=False)\n",
      "  (conv2): Conv1d(200, 100, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "  (dropout3): Dropout(p=0.5, inplace=False)\n",
      "  (conv3): Conv1d(100, 64, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "  (dropout4): Dropout(p=0.5, inplace=False)\n",
      "  (gru1): GRU(1, 8, batch_first=True, bidirectional=True)\n",
      "  (dropout5): Dropout(p=0.5, inplace=False)\n",
      "  (gru2): GRU(8, 16, batch_first=True, bidirectional=True)\n",
      "  (dropout6): Dropout(p=0.5, inplace=False)\n",
      "  (gru3): GRU(16, 32, batch_first=True, bidirectional=True)\n",
      "  (dropout7): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (fc3): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = SMSClassifier(path=\"../../models/cnn_gru.pt\").to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s53O3zbN5cLQ"
   },
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "id": "J4CI5zMMtYNi",
    "outputId": "909de612-5c35-424f-bde6-7339e1d252be"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #245:  81% 244/300 [01:49<00:25,  2.23it/s, Training Loss = 0.013033062444264942, Validiation Loss = 0.11628417391330004]"
     ]
    }
   ],
   "source": [
    "model.fit([train_loader, valid_loader], 300, AdamW(model.parameters(), lr=1e-3), BCELoss())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3PV9BGT5hgM"
   },
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348
    },
    "id": "yivAd9HD205B",
    "outputId": "f8470188-159f-48fe-eeb0-fe263910e77f"
   },
   "outputs": [],
   "source": [
    "acc, pre, re, pr = model.evaluate(train_loader)\n",
    "model.saveResults(\"pytorch\", \"cnn_gru\", \"train\", acc, pre, re, pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348
    },
    "id": "gvSVJB5x5ka3",
    "outputId": "f0a50106-11e3-4e3c-fcbc-1cc2f02efc6f"
   },
   "outputs": [],
   "source": [
    "acc, pre, re, pr = model.evaluate(valid_loader)\n",
    "model.saveResults(\"pytorch\", \"cnn_gru\", \"valid\", acc, pre, re, pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348
    },
    "id": "z7KmNXPIqq1B",
    "outputId": "c48c3d7c-c251-411b-edf9-4dd978a454df"
   },
   "outputs": [],
   "source": [
    "acc, pre, re, pr = model.evaluate(test_loader)\n",
    "model.saveResults(\"pytorch\", \"cnn_gru\", \"test\", acc, pre, re, pr)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
